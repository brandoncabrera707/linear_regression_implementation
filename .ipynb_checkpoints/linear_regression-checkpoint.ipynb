{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c0cc11-eb2f-4484-aba7-2ef78775ea3b",
   "metadata": {},
   "source": [
    "Let's make the necessary imports for our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "761ed531-96b7-40e4-a3d5-b1ab1077489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fb1cc62c-9981-49fc-b77e-3845c969821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>statezip</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>313000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1340</td>\n",
       "      <td>7912</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1340</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>2005</td>\n",
       "      <td>18810 Densmore Ave N</td>\n",
       "      <td>Shoreline</td>\n",
       "      <td>WA 98133</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>2384000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3650</td>\n",
       "      <td>9050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3370</td>\n",
       "      <td>280</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>709 W Blaine St</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA 98119</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1930</td>\n",
       "      <td>11947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>1966</td>\n",
       "      <td>0</td>\n",
       "      <td>26206-26214 143rd Ave SE</td>\n",
       "      <td>Kent</td>\n",
       "      <td>WA 98042</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2000</td>\n",
       "      <td>8030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>857 170th Pl NE</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA 98008</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>550000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1940</td>\n",
       "      <td>10500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1140</td>\n",
       "      <td>800</td>\n",
       "      <td>1976</td>\n",
       "      <td>1992</td>\n",
       "      <td>9105 170th Ave NE</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA 98052</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "0  2014-05-02 00:00:00   313000.0       3.0       1.50         1340      7912   \n",
       "1  2014-05-02 00:00:00  2384000.0       5.0       2.50         3650      9050   \n",
       "2  2014-05-02 00:00:00   342000.0       3.0       2.00         1930     11947   \n",
       "3  2014-05-02 00:00:00   420000.0       3.0       2.25         2000      8030   \n",
       "4  2014-05-02 00:00:00   550000.0       4.0       2.50         1940     10500   \n",
       "\n",
       "   floors  waterfront  view  condition  sqft_above  sqft_basement  yr_built  \\\n",
       "0     1.5           0     0          3        1340              0      1955   \n",
       "1     2.0           0     4          5        3370            280      1921   \n",
       "2     1.0           0     0          4        1930              0      1966   \n",
       "3     1.0           0     0          4        1000           1000      1963   \n",
       "4     1.0           0     0          4        1140            800      1976   \n",
       "\n",
       "   yr_renovated                    street       city  statezip country  \n",
       "0          2005      18810 Densmore Ave N  Shoreline  WA 98133     USA  \n",
       "1             0           709 W Blaine St    Seattle  WA 98119     USA  \n",
       "2             0  26206-26214 143rd Ave SE       Kent  WA 98042     USA  \n",
       "3             0           857 170th Pl NE   Bellevue  WA 98008     USA  \n",
       "4          1992         9105 170th Ave NE    Redmond  WA 98052     USA  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_data = pd.read_csv(\"data.csv\")\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d52b3-9288-4db3-bbdd-b4093a0ba4c5",
   "metadata": {},
   "source": [
    "First, we are going to implement simple linear regression.\n",
    "We will use the prices as our response and bedrooms to be our features\n",
    "\n",
    "So, following the formula for simple linear regression, our model is going to represent:\n",
    " $$price = \\beta_0 + \\beta_1 \\cdot \\, bedrooms \\, + \\epsilon_i $$\n",
    "\n",
    " where $$ \\epsilon $$ is our irreducible error term but can be ommitted since we don't know the true values for this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a4c9b-25e0-4a21-b4fc-538879211429",
   "metadata": {},
   "source": [
    "It is common practice to represent the regression model via matrix multiplication as it is easier to work with so now need to make a design matrix in the form of\n",
    "$$ y = X \\beta  $$\n",
    "where $$ X $$ represents a design matrix containing rows of ones for the first column which accounts for intercept/bias term and all subsquent columns are our features\n",
    "and $$\\beta$$ is a vector containing our model's weights, one for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c7e6263-88e2-4147-9fa8-6ae6509bc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'intercept' : np.ones(house_data.shape[0]), 'bedrooms' : house_data['bedrooms'], \n",
    "                  'bathrooms' : house_data['bathrooms']})\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6580276-c8c5-4bdc-a274-06f65198fc65",
   "metadata": {},
   "source": [
    "Now that we have a design matrix our next goal is to get estimates for our weights via gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a2b20-1d3f-4889-a42b-46188cb0cb14",
   "metadata": {},
   "source": [
    "In order to use gradient descent we have to define our cost function. We will use MSE, mean squared error, which is defined as:\n",
    "$$\n",
    "\\text{MSE}(\\beta) = \\frac{1}{n} \\| X\\beta - y \\|^2\n",
    "$$\n",
    "This is the matrix form of MSE and not the standard formula you would see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267af2d4-f11c-4d5b-848f-6b75eac89012",
   "metadata": {},
   "source": [
    "We must take the gradient with respect to $$\\beta$$\n",
    "which ends up looking like:\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = \\frac{1}{n} \\mathbf{X}^\\top (\\mathbf{X} \\boldsymbol{\\beta} - \\mathbf{y})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0804c0-5e56-443c-b6a3-52e01f0758de",
   "metadata": {},
   "source": [
    "Let's define a function for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1b82112-20bd-4fd5-9344-8c8e66c3987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, X, beta, n, alpha, iterations):\n",
    "    for i in range(0, 100):\n",
    "        gradient =  X.T @ (X @ beta - y)/ n\n",
    "        beta = beta - alpha * gradient\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974c640-c8a7-45f6-b5e6-8dac23a118c8",
   "metadata": {},
   "source": [
    "Let's define the necessary variables to use the gradient_descent function, our response is going to be our prices and we'll initalize our weights to random values. We are also going to split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bdeecde-96c5-47ec-87cf-714b7fb11794",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = house_data['price']\n",
    "y = y.to_numpy()\n",
    "beta = np.ones(X.shape[1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "n = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "94068114-b590-42b7-a237-302958c6d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = gradient_descent(y_train, X_train, beta, n, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ff6df4b-73c9-4f45-9003-2cfb1525309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1639.4363617   22469.94994039 220044.25999952]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f69acb-3387-4d54-98e1-8e2f518b0003",
   "metadata": {},
   "source": [
    "It's hard to tell how many iterations we should use for gradient descent, so to help gauge how many iterations we need, we can take a look at our cost function every couple of iterations of our choosing. \n",
    "If we see that the cost function is not decreasing by a certain threshold of our liking, then we can put a stop to the iterations early, reducing the amount of \n",
    "compute used.\n",
    "So let's define the cost function in this case, MSE.\n",
    "Afterwards we will update our gradient descent function to include a stopping condition using the MSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "63b03303-fb5b-4b99-aa36-da2ee8533b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, X, beta, n):\n",
    "    mse = ((X @ beta - y).T @ (X @ beta - y)) / n\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "847c2a72-28ea-4ae9-ad24-923c6867f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: MSE = 423321595661.6596, MSE decreased by = 1573807.330444\n",
      "Iteration 100: MSE = 423314630111.8835, MSE decreased by = 6965549.776062\n",
      "Iteration 200: MSE = 423312349750.8951, MSE decreased by = 2280360.988403\n",
      "Iteration 300: MSE = 423311256831.1976, MSE decreased by = 1092919.697571\n",
      "Iteration 400: MSE = 423310651966.1788, MSE decreased by = 604865.018738\n",
      "Iteration 500: MSE = 423310263571.7040, MSE decreased by = 388394.474854\n",
      "Iteration 600: MSE = 423309983438.6240, MSE decreased by = 280133.080017\n",
      "Iteration 700: MSE = 423309766207.7997, MSE decreased by = 217230.824219\n",
      "Iteration 800: MSE = 423309591078.8568, MSE decreased by = 175128.942993\n",
      "Iteration 900: MSE = 423309447161.5002, MSE decreased by = 143917.356506\n",
      "Iteration 1000: MSE = 423309327819.0140, MSE decreased by = 119342.486267\n",
      "Iteration 1100: MSE = 423309228440.5083, MSE decreased by = 99378.505676\n",
      "Iteration 1200: MSE = 423309145527.7907, MSE decreased by = 82912.717590\n",
      "Iteration 1300: MSE = 423309076292.2950, MSE decreased by = 69235.495667\n",
      "Iteration 1400: MSE = 423309018454.8802, MSE decreased by = 57837.414795\n",
      "Iteration 1500: MSE = 423308970130.3876, MSE decreased by = 48324.492676\n",
      "Iteration 1600: MSE = 423308929750.8499, MSE decreased by = 40379.537659\n",
      "Iteration 1700: MSE = 423308896008.7933, MSE decreased by = 33742.056580\n",
      "Iteration 1800: MSE = 423308867812.6891, MSE decreased by = 28196.104187\n",
      "Iteration 1900: MSE = 423308844250.8060, MSE decreased by = 23561.883179\n",
      "Iteration 2000: MSE = 423308824561.4093, MSE decreased by = 19689.396667\n",
      "Iteration 2100: MSE = 423308808108.0151, MSE decreased by = 16453.394226\n",
      "Iteration 2200: MSE = 423308794358.7682, MSE decreased by = 13749.246887\n",
      "Iteration 2300: MSE = 423308782869.2329, MSE decreased by = 11489.535278\n",
      "Iteration 2400: MSE = 423308773268.0205, MSE decreased by = 9601.212402\n",
      "Iteration 2500: MSE = 423308765244.7816, MSE decreased by = 8023.238892\n",
      "Iteration 2600: MSE = 423308758540.1738, MSE decreased by = 6704.607849\n",
      "Iteration 2700: MSE = 423308752937.4780, MSE decreased by = 5602.695801\n",
      "Iteration 2800: MSE = 423308748255.5933, MSE decreased by = 4681.884705\n",
      "Iteration 2900: MSE = 423308744343.1832, MSE decreased by = 3912.410095\n",
      "Iteration 3000: MSE = 423308741073.7833, MSE decreased by = 3269.399902\n",
      "Iteration 3100: MSE = 423308738341.7140, MSE decreased by = 2732.069275\n",
      "Iteration 3200: MSE = 423308736058.6641, MSE decreased by = 2283.049866\n",
      "Iteration 3300: MSE = 423308734150.8368, MSE decreased by = 1907.827332\n",
      "Iteration 3400: MSE = 423308732556.5638, MSE decreased by = 1594.273010\n",
      "Iteration 3500: MSE = 423308731224.3117, MSE decreased by = 1332.252075\n",
      "Iteration 3600: MSE = 423308730111.0172, MSE decreased by = 1113.294495\n",
      "Iteration 3700: MSE = 423308729180.6943, MSE decreased by = 930.322937\n",
      "Iteration 3800: MSE = 423308728403.2712, MSE decreased by = 777.423035\n",
      "Iteration 3900: MSE = 423308727753.6189, MSE decreased by = 649.652344\n",
      "Iteration 4000: MSE = 423308727210.7377, MSE decreased by = 542.881165\n",
      "Iteration 4100: MSE = 423308726757.0798, MSE decreased by = 453.657898\n",
      "Iteration 4200: MSE = 423308726377.9813, MSE decreased by = 379.098511\n",
      "Iteration 4300: MSE = 423308726061.1883, MSE decreased by = 316.793030\n",
      "Iteration 4400: MSE = 423308725796.4605, MSE decreased by = 264.727783\n",
      "Iteration 4500: MSE = 423308725575.2412, MSE decreased by = 221.219299\n",
      "Iteration 4600: MSE = 423308725390.3795, MSE decreased by = 184.861755\n",
      "Iteration 4700: MSE = 423308725235.9003, MSE decreased by = 154.479126\n",
      "Iteration 4800: MSE = 423308725106.8097, MSE decreased by = 129.090637\n",
      "Iteration 4900: MSE = 423308724998.9356, MSE decreased by = 107.874084\n",
      "Iteration 5000: MSE = 423308724908.7906, MSE decreased by = 90.145020\n",
      "Iteration 5100: MSE = 423308724833.4611, MSE decreased by = 75.329468\n",
      "Iteration 5200: MSE = 423308724770.5121, MSE decreased by = 62.948975\n",
      "Iteration 5300: MSE = 423308724717.9090, MSE decreased by = 52.603149\n",
      "Iteration 5400: MSE = 423308724673.9510, MSE decreased by = 43.958008\n",
      "Iteration 5500: MSE = 423308724637.2178, MSE decreased by = 36.733215\n",
      "Iteration 5600: MSE = 423308724606.5217, MSE decreased by = 30.696106\n",
      "Iteration 5700: MSE = 423308724580.8705, MSE decreased by = 25.651123\n",
      "Iteration 5800: MSE = 423308724559.4351, MSE decreased by = 21.435425\n",
      "Iteration 5900: MSE = 423308724541.5227, MSE decreased by = 17.912415\n",
      "Iteration 6000: MSE = 423308724526.5543, MSE decreased by = 14.968445\n",
      "Iteration 6100: MSE = 423308724514.0458, MSE decreased by = 12.508423\n",
      "Iteration 6200: MSE = 423308724503.5931, MSE decreased by = 10.452698\n",
      "Iteration 6300: MSE = 423308724494.8585, MSE decreased by = 8.734680\n",
      "Iteration 6400: MSE = 423308724487.5594, MSE decreased by = 7.299011\n",
      "Iteration 6500: MSE = 423308724481.4598, MSE decreased by = 6.099609\n",
      "Iteration 6600: MSE = 423308724476.3629, MSE decreased by = 5.096985\n",
      "Iteration 6700: MSE = 423308724472.1033, MSE decreased by = 4.259521\n",
      "Iteration 6800: MSE = 423308724468.5439, MSE decreased by = 3.559387\n",
      "Iteration 6900: MSE = 423308724465.5698, MSE decreased by = 2.974182\n",
      "Iteration 7000: MSE = 423308724463.0842, MSE decreased by = 2.485535\n",
      "Iteration 7100: MSE = 423308724461.0071, MSE decreased by = 2.077087\n",
      "Iteration 7200: MSE = 423308724459.2716, MSE decreased by = 1.735535\n",
      "Iteration 7300: MSE = 423308724457.8210, MSE decreased by = 1.450562\n",
      "Iteration 7400: MSE = 423308724456.6091, MSE decreased by = 1.211914\n",
      "Iteration 7500: MSE = 423308724455.5964, MSE decreased by = 1.012756\n",
      "Iteration 7600: MSE = 423308724454.7500, MSE decreased by = 0.846375\n",
      "Iteration 7700: MSE = 423308724454.0427, MSE decreased by = 0.707275\n",
      "Iteration 7800: MSE = 423308724453.4517, MSE decreased by = 0.591003\n",
      "Iteration 7900: MSE = 423308724452.9578, MSE decreased by = 0.493958\n",
      "Iteration 8000: MSE = 423308724452.5451, MSE decreased by = 0.412659\n",
      "Iteration 8100: MSE = 423308724452.2001, MSE decreased by = 0.344971\n",
      "Iteration 8200: MSE = 423308724451.9119, MSE decreased by = 0.288208\n",
      "Iteration 8300: MSE = 423308724451.6711, MSE decreased by = 0.240784\n",
      "Iteration 8400: MSE = 423308724451.4699, MSE decreased by = 0.201233\n",
      "Iteration 8500: MSE = 423308724451.3018, MSE decreased by = 0.168152\n",
      "Iteration 8600: MSE = 423308724451.1613, MSE decreased by = 0.140503\n",
      "Iteration 8700: MSE = 423308724451.0437, MSE decreased by = 0.117554\n",
      "Iteration 8800: MSE = 423308724450.9455, MSE decreased by = 0.098206\n",
      "Iteration 8900: MSE = 423308724450.8636, MSE decreased by = 0.081909\n",
      "Iteration 9000: MSE = 423308724450.7951, MSE decreased by = 0.068481\n",
      "Iteration 9100: MSE = 423308724450.7377, MSE decreased by = 0.057373\n",
      "Iteration 9200: MSE = 423308724450.6898, MSE decreased by = 0.047913\n",
      "Iteration 9300: MSE = 423308724450.6500, MSE decreased by = 0.039856\n",
      "Iteration 9400: MSE = 423308724450.6165, MSE decreased by = 0.033508\n",
      "Iteration 9500: MSE = 423308724450.5886, MSE decreased by = 0.027893\n",
      "Iteration 9600: MSE = 423308724450.5653, MSE decreased by = 0.023254\n",
      "Iteration 9700: MSE = 423308724450.5457, MSE decreased by = 0.019653\n",
      "Iteration 9800: MSE = 423308724450.5294, MSE decreased by = 0.016235\n",
      "Iteration 9900: MSE = 423308724450.5159, MSE decreased by = 0.013550\n",
      "Iteration 10000: MSE = 423308724450.5045, MSE decreased by = 0.011414\n",
      "Iteration 10100: MSE = 423308724450.4949, MSE decreased by = 0.009583\n",
      "Iteration 10200: MSE = 423308724450.4870, MSE decreased by = 0.007874\n",
      "Iteration 10300: MSE = 423308724450.4803, MSE decreased by = 0.006653\n",
      "Iteration 10400: MSE = 423308724450.4748, MSE decreased by = 0.005554\n",
      "Iteration 10500: MSE = 423308724450.4702, MSE decreased by = 0.004639\n",
      "Iteration 10600: MSE = 423308724450.4662, MSE decreased by = 0.003906\n",
      "Iteration 10700: MSE = 423308724450.4630, MSE decreased by = 0.003235\n",
      "Iteration 10800: MSE = 423308724450.4603, MSE decreased by = 0.002686\n",
      "Iteration 10900: MSE = 423308724450.4581, MSE decreased by = 0.002258\n",
      "Iteration 11000: MSE = 423308724450.4562, MSE decreased by = 0.001892\n",
      "Iteration 11100: MSE = 423308724450.4546, MSE decreased by = 0.001587\n",
      "Iteration 11200: MSE = 423308724450.4532, MSE decreased by = 0.001343\n",
      "Iteration 11300: MSE = 423308724450.4523, MSE decreased by = 0.000916\n",
      "Iteration 11400: MSE = 423308724450.4514, MSE decreased by = 0.000977\n",
      "Iteration 11500: MSE = 423308724450.4504, MSE decreased by = 0.000916\n",
      "Iteration 11600: MSE = 423308724450.4499, MSE decreased by = 0.000549\n",
      "Iteration 11700: MSE = 423308724450.4494, MSE decreased by = 0.000488\n",
      "Iteration 11800: MSE = 423308724450.4490, MSE decreased by = 0.000427\n",
      "Iteration 11900: MSE = 423308724450.4484, MSE decreased by = 0.000549\n",
      "Iteration 12000: MSE = 423308724450.4482, MSE decreased by = 0.000244\n",
      "Iteration 12100: MSE = 423308724450.4479, MSE decreased by = 0.000244\n",
      "Iteration 12200: MSE = 423308724450.4477, MSE decreased by = 0.000244\n",
      "Iteration 12300: MSE = 423308724450.4476, MSE decreased by = 0.000122\n",
      "Iteration 12400: MSE = 423308724450.4473, MSE decreased by = 0.000244\n",
      "Iteration 12500: MSE = 423308724450.4473, MSE decreased by = 0.000061\n",
      "Iteration 12600: MSE = 423308724450.4471, MSE decreased by = 0.000183\n",
      "Iteration 12700: MSE = 423308724450.4471, MSE decreased by = 0.000000\n",
      "Early stopping at iteration 12700 — MSE improvement below threshold (1e-05)\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(y, X, beta, n, alpha, iterations, tolerance=1e-5, check_every=100):\n",
    "    prev_mse = mean_squared_error(y, X, beta, n)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradient = (1 / n) * X.T @ (X @ beta - y)\n",
    "        beta = beta - alpha * gradient\n",
    "\n",
    "        # Check MSE change every check_every steps\n",
    "        if i % check_every == 0:\n",
    "            current_mse = mean_squared_error(y, X, beta, n)\n",
    "            improvement = abs(prev_mse - current_mse)\n",
    "\n",
    "            print(f\"Iteration {i}: MSE = {current_mse:.4f}, MSE decreased by = {improvement:.6f}\")\n",
    "\n",
    "            if improvement < tolerance:\n",
    "                print(f\"Early stopping at iteration {i} — MSE improvement below threshold ({tolerance})\")\n",
    "                return beta\n",
    "\n",
    "            prev_mse = current_mse  # Update for next check\n",
    "\n",
    "    print(f\"Completed full {iterations} iterations. Final MSE: {mean_squared_error(y, X, beta, n):.4f}\")\n",
    "    return beta\n",
    "beta = gradient_descent(y,X,beta, n, 0.01, 100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e96e3-36ef-4740-8afb-aca5069885f4",
   "metadata": {},
   "source": [
    "Now we can finally do our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34889219-3399-4889-bcb2-a37ae9b9825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4391.97937421  19231.34338782 223141.57667692]\n"
     ]
    }
   ],
   "source": [
    "print(beta)\n",
    "predictions = X_test @ beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ada2c8a9-7deb-42ed-b99b-862e7ad22b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[619939.95122996 602617.24383637 787296.13373765 ... 658402.6380056\n",
      " 471815.11211009 658402.6380056 ]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b950988-4675-40af-b5d1-49ca905a005d",
   "metadata": {},
   "source": [
    "Now we can use Scikit-learn's MAE function to see how our model did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4fee93f2-5dd0-49af-a332-916a38c1c721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230680.8267999549"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac3d3a-01f2-4f26-b310-12e1fb4474a6",
   "metadata": {},
   "source": [
    "This value indicates that, on average, our model is off from the true values by the above amount. This means our model is not particulary performing well. However, we completed our goal of implementing a basic implementation of linear regression using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fb642-f2f2-42f0-adc1-14e6625bc2a1",
   "metadata": {},
   "source": [
    "If we wanted to further improve this model, we would include more features and make sure our implementation can handle that. We would through the process of feature selection determing what features effect the model the most and build a model with those features. It would be worth scaling our data since our features have varying scales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
